{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba3b221-970b-41e1-9a41-697d44db497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in dir():\n",
    "    if not name.startswith('_'):\n",
    "        del globals()[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908546e2-1184-42b7-8b0e-d2b72ab62cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import PIL\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt # Ploting the diagrams\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # Ploting the diagrams\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras import models\n",
    "import random\n",
    "from keras.models import model_from_json\n",
    "import pandas as pd\n",
    "tf.random.set_seed(333)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3f5cad-68b3-481d-8965-d4e3e5fc5266",
   "metadata": {},
   "source": [
    "## 1. Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc7781b-397b-406a-841d-f3fdc4d4e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = 5\n",
    "rounds = 12\n",
    "splits = (clients+1)*rounds+1\n",
    "\n",
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61c106d-efd9-4642-b483-516fb352fce8",
   "metadata": {},
   "source": [
    "## 2. Loading dataset in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c82101a-9ebb-426a-add1-4b8c31830599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genImageList(l):\n",
    "    return list(filter(lambda i: i.split(\".\")[-1] in [\"png\",\"jpg\",\"jpeg\"], l))\n",
    "def genDf(images,labels):\n",
    "    df= pd.DataFrame(images,columns=[\"Image\"])\n",
    "    df[\"Class\"] = labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b98e16-3930-4028-9b48-9e36181a4d69",
   "metadata": {},
   "source": [
    "## 2.1 Generating Global Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daab15e7-77f3-4326-96cf-00dc3fc0b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting all FileNames\n",
    "gMaskPath = r'Test/Mask/'\n",
    "gNoMaskPath = r'Test/NoMask/'\n",
    "gMaskImages = genImageList([gMaskPath+i for i in os.listdir(gMaskPath)])\n",
    "gMaskLabels = np.zeros(len(gMaskImages))\n",
    "gNoMaskImages = genImageList([gNoMaskPath+i for i in os.listdir(gNoMaskPath)])\n",
    "gNoMaskLabels = np.ones(len(gNoMaskImages))\n",
    "\n",
    "gMaskDf = genDf(gMaskImages,gMaskLabels)\n",
    "gNoMaskDf = genDf(gNoMaskImages,gNoMaskLabels)\n",
    "\n",
    "gDf = pd.DataFrame(shuffle(pd.concat([gMaskDf,gNoMaskDf])),columns=gMaskDf.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7e5429-d80c-4fc1-b19b-08fa1d3d0791",
   "metadata": {},
   "source": [
    "## 3. Stratified Kfolds Custom Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "070a80eb-6ff0-46c5-8f7b-7bab4747112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genKSplitDf(df,splits):\n",
    "    index = np.array_split(shuffle(df.index,random_state=3333),splits)    \n",
    "    folds = []\n",
    "    for i in range(splits):\n",
    "        folds.append(df.filter(items = index[i], axis=0))\n",
    "    return folds\n",
    "def mergeDf(df1,df2,splits):\n",
    "    columns = df1[0].columns\n",
    "    folds = []\n",
    "    for i in range(splits):\n",
    "        folds.append(pd.DataFrame(shuffle(pd.concat([df1[i],df2[i]]),random_state=3333),columns=columns)) \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16b06cd9-6aae-4d23-b8cc-e7dbf0138d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "maskFolds = genKSplitDf(gMaskDf,splits)\n",
    "noMaskFolds = genKSplitDf(gNoMaskDf,splits)\n",
    "folds = mergeDf(maskFolds,noMaskFolds,splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "855d6bd9-5d2c-42aa-bd86-e4daf96e5147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c352e30e-851c-410f-936c-495b46cbf808",
   "metadata": {},
   "source": [
    "## 4. Simulation Setup for Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "010aa12c-f386-4248-a2ef-f68d2a8b681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history,plot_name):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1,figsize=(5,5))\n",
    "    ax.plot(history.history['loss'])\n",
    "    ax.plot(history.history['val_loss'])\n",
    "    ax.set_title('Model Loss')\n",
    "    ax.set(xlabel='Epoch',ylabel='Loss')\n",
    "    ax.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(\"History/\"+plot_name+\".png\")\n",
    "def createModel(shape=(100,100,3)):\n",
    "    tf.random.set_seed(3333)\n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  # loss='kl_divergence',\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics=[tf.keras.metrics.Recall(),tf.keras.metrics.Precision(),tf.keras.metrics.AUC(\n",
    "        num_thresholds=200, curve='ROC',\n",
    "        summation_method='interpolation', name=None, dtype=None,\n",
    "        thresholds=None, multi_label=False, label_weights=None\n",
    "    ),'accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def genModel(df,model,name):\n",
    "    \n",
    "    x,y = df[\"Image\"],df[\"Class\"]\n",
    "    data = []\n",
    "    size = 100\n",
    "    \n",
    "    for img_path in x:\n",
    "        image = load_img(img_path, target_size=(size, size))\n",
    "        image = img_to_array(image)\n",
    "        image = image/255\n",
    "        data.append(image)\n",
    "\n",
    "    data = np.array(data, dtype=\"float32\")\n",
    "    labels = np.array(y).reshape([-1,1])\n",
    "    \n",
    "    # trainx,testx,trainy,testy = train_test_split(data,labels,random_state=333,test_size=0.3)\n",
    "    \n",
    "    history = model.fit(data, labels, epochs=10,validation_split=0.2,verbose=0,callbacks=[tensorboard_callback])\n",
    "    acc = []\n",
    "    loss = []\n",
    "    \n",
    "    for i,j in zip(history.history[\"loss\"],history.history[\"accuracy\"]):\n",
    "        acc.append(j)\n",
    "        loss.append(i)\n",
    "    \n",
    "    # plot_history(history,name)\n",
    "    return model,acc,loss\n",
    "\n",
    "def genXY(df):\n",
    "    x,y = df[\"Image\"],df[\"Class\"]\n",
    "    data = []\n",
    "    size = 100\n",
    "    \n",
    "    for img_path in x:\n",
    "        image = load_img(img_path, target_size=(size, size))\n",
    "        image = img_to_array(image)\n",
    "        image = image/255\n",
    "        data.append(image)\n",
    "\n",
    "    data = np.array(data, dtype=\"float32\")\n",
    "    labels = np.array(y).reshape([-1,1])\n",
    "    return data,labels\n",
    "\n",
    "class KLLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def call(self, y_true, y_pred):\n",
    "        KL = []\n",
    "        kl = tf.keras.losses.KLDivergence()\n",
    "        for i in y_pred:\n",
    "            KL.append(kl(y_true,i))\n",
    "        KL = tf.convert_to_tensor(KL, dtype=tf.float32)\n",
    "        return tf.reduce_mean(KL)\n",
    "def loss(model, x, y, training,preds):\n",
    "  # training=training is needed only if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    # print(x.shape)\n",
    "    y_ = model(x, training=training)\n",
    "    \n",
    "    ceLoss = tf.keras.losses.BinaryCrossentropy()(y,y_)\n",
    "    \n",
    "    loss_object = KLLoss()\n",
    "    \n",
    "    return ceLoss + loss_object(y_true=y_, y_pred=preds)\n",
    "\n",
    "def grad(model, inputs, targets,preds):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets,True,preds)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "def ceil(a,b):\n",
    "    return -(-a//b)\n",
    "\n",
    "def optimizeWeights(model,X,Y,preds):\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    # loss_value, grads = grad(model, x, y,preds)\n",
    "    # optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    batches = ceil(n_samples,32)\n",
    "    batchx = np.array_split(X,batches)\n",
    "    # print(batchx[0].shape)\n",
    "    batchy = np.array_split(Y,batches)\n",
    "    batchpreds = np.array_split(np.array(preds),batches,axis=1)\n",
    "    # print(batchpreds[0].shape)\n",
    "    num_epochs = 10\n",
    "    train_loss_results = []\n",
    "    train_accuracy_results = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        epoch_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "        \n",
    "      # Training loop - using batches of 32\n",
    "        c = 0\n",
    "        for x,y,z in zip(batchx,batchy,batchpreds):\n",
    "            # Optimize the model\n",
    "            loss_value, grads = grad(model, x, y,z)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            # Track progress\n",
    "            epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
    "            epoch_accuracy.update_state(y, model(x, training=True))\n",
    "            \n",
    "        train_loss_results.append(epoch_loss_avg.result())\n",
    "        train_accuracy_results.append(epoch_accuracy.result())\n",
    "        \n",
    "    accHistory = [i.numpy() for i in train_accuracy_results]\n",
    "    lossHistory = [i.numpy() for i in train_loss_results]\n",
    "    \n",
    "    return model,accHistory,lossHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128329f-7aba-48eb-aa03-b9e73e9696e9",
   "metadata": {},
   "source": [
    "## 4.1 Initializing Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7edd170-8473-4083-8cb1-9285f56b5136",
   "metadata": {},
   "outputs": [],
   "source": [
    "globalModel,initAcc,initLoss = genModel(folds.pop(),createModel(),\"Init\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2880c6-5d04-4610-bda4-0f1452b40ead",
   "metadata": {},
   "source": [
    "## 4.2 Initializing Cleint Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d57804d-211f-4d88-8893-e508758aa72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genClientModel(globalModel,name):\n",
    "    model = createModel()\n",
    "    model.set_weights(globalModel.get_weights())\n",
    "    model._name = name\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcdaecb9-a840-4343-aaa8-62e117a8e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientModels = [genClientModel(globalModel,\"Client_\"+str(i)) for i in range(clients)]\n",
    "modelRes = np.zeros(clients).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79d1f19e-57e3-4bf8-8f83-295dbe18d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientRes = {}\n",
    "clientLossHist = {}\n",
    "clientAccHist = {}\n",
    "for c in range(clients):\n",
    "    clientRes[c] = []\n",
    "    clientLossHist[c] = initLoss[:]\n",
    "    clientAccHist[c] =  initAcc[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4657dda8-6cfb-45a6-8bb1-60caf9100b50",
   "metadata": {},
   "source": [
    "## 4.3 Training Rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "531c119a-0f39-408e-98dc-6d9a356812e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round -  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yashb\\anaconda3\\envs\\goku\\lib\\site-packages\\PIL\\Image.py:976: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002A6A9323DC8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Round -  2\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Round -  3\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Round -  4\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Round -  5\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Round -  6\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Round -  7\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Round -  8\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Round -  9\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Round -  10\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Round -  11\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Round -  12\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "for i in range(rounds):\n",
    "    print(\"Round - \",i+1)\n",
    "    \n",
    "    for c in range(clients):\n",
    "        #Training All clients on separate folds\n",
    "        clientModels[c],acc,trainloss = genModel(folds.pop(),clientModels[c],\"C\"+str(c)+\"_\"+str(i)) \n",
    "        clientLossHist[c].extend(trainloss[:])\n",
    "        clientAccHist[c].extend(acc[:])  \n",
    "            \n",
    "        \n",
    "    #Aggregation stuff\n",
    "    x,y = genXY(folds.pop())\n",
    "    \n",
    "    preds = []\n",
    "    \n",
    "    #get Test Prediction\n",
    "    for c in range(clients):\n",
    "        eVal = clientModels[c].evaluate(x,y,verbose=0)\n",
    "        clientRes[c].append(eVal[-1])\n",
    "        clientLossHist[c].append(eVal[0])\n",
    "        clientAccHist[c].append(eVal[-1])\n",
    "        preds.append(clientModels[c].predict(x))\n",
    "        \n",
    "    for m in range(clients):\n",
    "        nOthers = [preds[i] for i in range(clients) if i != m] \n",
    "        clientModels[m],acc,trainLoss = optimizeWeights(clientModels[m],x,y,nOthers)\n",
    "        clientLossHist[m].extend(trainLoss[:])\n",
    "        clientAccHist[m].extend(acc[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0555e8e7-6a0a-4d50-b877-228019af71bd",
   "metadata": {},
   "source": [
    "## 5. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca4f25af-814b-41a8-b5b2-c0b312b280c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gMaskPath = r'Global/Mask/'\n",
    "gNoMaskPath = r'Global/NoMask/'\n",
    "gMaskImages = genImageList([gMaskPath+i for i in os.listdir(gMaskPath)])\n",
    "gMaskLabels = np.zeros(len(gMaskImages))\n",
    "gNoMaskImages = genImageList([gNoMaskPath+i for i in os.listdir(gNoMaskPath)])\n",
    "gNoMaskLabels = np.ones(len(gNoMaskImages))\n",
    "\n",
    "gMaskDf = genDf(gMaskImages,gMaskLabels)\n",
    "gNoMaskDf = genDf(gNoMaskImages,gNoMaskLabels)\n",
    "\n",
    "tDf = pd.DataFrame(shuffle(pd.concat([gMaskDf,gNoMaskDf])),columns=gMaskDf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "435f384a-8c69-4672-a01a-d628c4560136",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx,ty = genXY(tDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af34eb68-f38d-43ad-bc5c-cfd60e34948a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 1s 7ms/step - loss: 0.1668 - recall_1: 0.9506 - precision_1: 0.9474 - auc_1: 0.9786 - accuracy: 0.9489\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2031 - recall_2: 0.9656 - precision_2: 0.9233 - auc_2: 0.9638 - accuracy: 0.9427\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2112 - recall_3: 0.9559 - precision_3: 0.9341 - auc_3: 0.9670 - accuracy: 0.9442\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2201 - recall_4: 0.9582 - precision_4: 0.9324 - auc_4: 0.9693 - accuracy: 0.9444\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2353 - recall_5: 0.9479 - precision_5: 0.9369 - auc_5: 0.9656 - accuracy: 0.9421\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for c in range(clients):\n",
    "    res.append(clientModels[c].evaluate(tx,ty)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1319682d-8c56-4fa5-90a9-28324b271509",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"Testing Results\"] = res\n",
    "df.to_csv(\"Results/ml.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df94e1-7f50-45e3-ac16-ed06b13b8556",
   "metadata": {},
   "source": [
    "## 6. Analyzing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5874c2a2-2e95-47c9-8668-de078c64b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfL = pd.DataFrame()\n",
    "for k in clientLossHist.keys():\n",
    "    dfL[\"Client \"+ str(k)] = clientLossHist[k]\n",
    "dfL.to_csv(\"Results/mlLoss.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d71acdfc-2e62-48d5-9709-a3561c0d6b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for k in clientLossHist.keys():\n",
    "    df[\"Client \"+ str(k)] = clientAccHist[k]\n",
    "df.to_csv(\"Results/mlAcc.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
