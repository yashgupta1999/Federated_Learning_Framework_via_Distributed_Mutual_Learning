{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8fc28f9-263d-468d-b3ba-6a37987b4934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# For Jupyter notebooks\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import commonly used modules\n",
    "from data_utils import *\n",
    "from model_utils import *\n",
    "from config_utils import load_config\n",
    "\n",
    "# Load default config\n",
    "CONFIG = load_config('../config/fl_template_config.yaml')\n",
    "\n",
    "# Export commonly used items\n",
    "__all__ = ['CONFIG']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd135af-d3eb-45a2-b93e-80f411e335b5",
   "metadata": {},
   "source": [
    "## FedAvg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa655466-f5d4-4223-807f-e9e0313fb255",
   "metadata": {},
   "source": [
    "### Federated Learning Averaging Pseudocode\n",
    "\n",
    "### Server Initialization:\n",
    "Initialize global model weights W₀\n",
    "\n",
    "### Main Federated Learning Loop:\n",
    "For each round t = 1 to T:\n",
    "    \n",
    "    1. Select a subset of clients Sₜ (or use all available clients)\n",
    "    \n",
    "    2. Broadcast the current global model weights Wₜ to all clients in Sₜ\n",
    "\n",
    "    3. For each client k in Sₜ (executed in parallel):\n",
    "         - Perform a local update:\n",
    "           Wₜᵏ = ClientUpdate(Wₜ, local_dataₖ)\n",
    "         - Return updated local model weights Wₜᵏ along with the number of samples nₖ\n",
    "\n",
    "    4. Aggregate the updated weights using weighted averaging (FedAvg):\n",
    "         - Compute total samples: N = Σₖ₍∈Sₜ₎ nₖ\n",
    "         - Update global model weights:\n",
    "           Wₜ₊₁ = Σₖ₍∈Sₜ₎ (nₖ / N) * Wₜᵏ\n",
    "\n",
    "Return final global model weights W_T\n",
    "\n",
    "### ClientUpdate Function:\n",
    "Function ClientUpdate(W, local_data):\n",
    "    \n",
    "    - Set W_local = W\n",
    "    \n",
    "    - For each local epoch e = 1 to E:\n",
    "         - For each batch b in local_data:\n",
    "              - Compute gradient: grad = ∇(loss(W_local, b))\n",
    "              - Update local weights: W_local = W_local - learning_rate * grad\n",
    "              \n",
    "    Return W_local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7276035-ce5b-41e2-a429-a9c4557b2a82",
   "metadata": {},
   "source": [
    "#### Central Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9c36563-db7a-4182-844d-53ca7545e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd69ae14-952a-402d-8ab0-27920d8c7b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "trainx,trainy = load_training_data(f'../experiments/{CONFIG['experiment_name']}/processed_data/init.npy')\n",
    "histroy =  train_model(model,trainx,trainy)\n",
    "history_dic['init'] = history\n",
    "save_model(f'../experiments/{CONFIG['experiment_name']}/models/central_model.keras')\n",
    "clear_model_from_memory(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310839d-f271-4020-8163-19e1f2afb120",
   "metadata": {},
   "source": [
    "#### Client Models Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31dfff-d15f-48e2-8654-13f0ef91172c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## FedAvg Conditioned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988cea4-dd7a-41a2-a711-227a409e8094",
   "metadata": {},
   "source": [
    "### Modified FedAvg with Performance-based Weighting\n",
    "\n",
    "### Server Initialization:\n",
    "Initialize global model weights W₀\n",
    "\n",
    "### Main Federated Learning Loop:\n",
    "\n",
    "For each round t = 1 to T:\n",
    "\n",
    "    1. Select a subset of clients Sₜ (or use all available clients)\n",
    "    2. Broadcast the current global model weights Wₜ to all clients in Sₜ\n",
    "\n",
    "    3. For each client k in Sₜ (executed in parallel):\n",
    "         - Perform a local update:\n",
    "           Wₜᵏ = ClientUpdate(Wₜ, local_dataₖ)\n",
    "         - Evaluate the updated model on a common validation set:\n",
    "           aₖ = Evaluate(Wₜᵏ, validation_set)  # e.g., accuracy\n",
    "         - Return updated model Wₜᵏ, number of samples nₖ, and accuracy aₖ\n",
    "\n",
    "    4. Aggregate the updated weights:\n",
    "         - Compute the performance-weighted sum of samples:\n",
    "           Total_weight = Σₖ₍∈Sₜ₎ (nₖ × aₖ)\n",
    "         - Update global model weights:\n",
    "           Wₜ₊₁ = Σₖ₍∈Sₜ₎ [(nₖ × aₖ) / Total_weight] × Wₜᵏ\n",
    "\n",
    "Return final global model weights W_T\n",
    "\n",
    "### ClientUpdate Function:\n",
    "\n",
    "Function ClientUpdate(W, local_data):\n",
    "    \n",
    "    Set W_local = W\n",
    "    For each local epoch e = 1 to E:\n",
    "         For each batch b in local_data:\n",
    "              - Compute gradient: grad = ∇(loss(W_local, b))\n",
    "              - Update local weights: W_local = W_local - learning_rate * grad\n",
    "    Return W_local\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31356d21-8a4a-441d-a64a-cfff1fdcae51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Asynchronous Weight Updating Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f33ca41-ee95-4c42-9fa3-ed6f7bfa0897",
   "metadata": {},
   "source": [
    "### Federated Learning with Partial Weight Sharing (Deep Layers Updated Frequently)\n",
    "\n",
    "### Server Initialization:\n",
    "Initialize global shallow weights W_shallow₀\n",
    "Initialize global deep weights W_deep₀\n",
    "Set shallow_update_interval K  # e.g., update shallow layers every K rounds, update deep layers every round\n",
    "\n",
    "### Main Federated Learning Loop:\n",
    "\n",
    "For each round t = 1 to T:\n",
    "\n",
    "    1. Determine if this is a shallow update round:\n",
    "         If (t mod K == 0):\n",
    "             shallow_update = True\n",
    "         Else:\n",
    "             shallow_update = False\n",
    "\n",
    "    2. Client Selection & Broadcast:\n",
    "         Select a subset of clients Sₜ\n",
    "         For each client in Sₜ, send:\n",
    "             - Current deep weights: W_deepₜ  (always sent)\n",
    "             - If shallow_update is True, also send current shallow weights: W_shallowₜ\n",
    "             - Otherwise, clients use their locally stored shallow weights\n",
    "\n",
    "    3. Clients' Local Update (executed in parallel):\n",
    "         For each client k in Sₜ:\n",
    "             - If shallow_update is True:\n",
    "                  (W_shallowₜ^k, W_deepₜ^k) = ClientUpdate(W_shallowₜ, W_deepₜ, local_dataₖ, update_shallow=True)\n",
    "             - Else:\n",
    "                  (W_shallow_local, W_deepₜ^k) = ClientUpdate(W_shallow_local, W_deepₜ, local_dataₖ, update_shallow=False)\n",
    "             - Evaluate the full updated model on a common validation set:\n",
    "                  aₖ = Evaluate(FullModel(W_shallow, W_deep), validation_set)  # e.g., accuracy\n",
    "             - Return to server:\n",
    "                  - For shallow layers: if update_shallow is True, return updated W_shallowₜ^k; otherwise, no update (or the previous version)\n",
    "                  - Updated deep weights: W_deepₜ^k\n",
    "                  - Local sample count nₖ and performance metric aₖ\n",
    "\n",
    "    4. Server Aggregation:\n",
    "         # Always aggregate deep layers:\n",
    "         Compute Total_weight_deep = Σₖ₍∈Sₜ₎ (nₖ × aₖ)\n",
    "         Update global deep weights:\n",
    "             W_deepₜ₊₁ = Σₖ₍∈Sₜ₎ [ (nₖ × aₖ) / Total_weight_deep ] × W_deepₜ^k\n",
    "\n",
    "         # Aggregate shallow layers only on shallow update rounds:\n",
    "         If shallow_update is True:\n",
    "             Compute Total_weight_shallow = Σₖ₍∈Sₜ₎ (nₖ × aₖ)\n",
    "             Update global shallow weights:\n",
    "                 W_shallowₜ₊₁ = Σₖ₍∈Sₜ₎ [ (nₖ × aₖ) / Total_weight_shallow ] × W_shallowₜ^k\n",
    "         Else:\n",
    "             W_shallowₜ₊₁ = W_shallowₜ  # Keep shallow layers unchanged\n",
    "\n",
    "    Return final global model: {W_shallow_T, W_deep_T}\n",
    "\n",
    "\n",
    "### ClientUpdate Function:\n",
    "\n",
    "Function ClientUpdate(shallow_weights, deep_weights, local_data, update_shallow):\n",
    "\n",
    "    If update_shallow is True:\n",
    "         Set local_shallow = shallow_weights    # Received from server\n",
    "    Else:\n",
    "         Set local_shallow = local_shallow      # Use previously stored shallow weights locally\n",
    "\n",
    "    Set local_deep = deep_weights              # Always use the latest deep weights from server\n",
    "\n",
    "    For each local epoch e = 1 to E:\n",
    "         For each batch b in local_data:\n",
    "              If update_shallow is True:\n",
    "                  - Compute gradients for both layers:\n",
    "                        grad_shallow, grad_deep = ∇(loss(FullModel(local_shallow, local_deep), b))\n",
    "                  - Update shallow layers:\n",
    "                        local_shallow = local_shallow - learning_rate * grad_shallow\n",
    "              Else:\n",
    "                  - Compute gradient only for deep layers (shallow remains fixed):\n",
    "                        grad_deep = ∇(loss(FullModel(local_shallow, local_deep), b))\n",
    "              - Update deep layers:\n",
    "                    local_deep = local_deep - learning_rate * grad_deep\n",
    "\n",
    "    If update_shallow is True:\n",
    "         Return (local_shallow, local_deep)\n",
    "    Else:\n",
    "         Return (local_shallow, local_deep)  # Note: shallow remains unchanged from before the round\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
