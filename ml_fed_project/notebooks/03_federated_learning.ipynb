{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fc28f9-263d-468d-b3ba-6a37987b4934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 13:32:54.172018: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742931174.189427    5842 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742931174.194730    5842 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742931174.213034    5842 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742931174.213064    5842 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742931174.213066    5842 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742931174.213068    5842 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-25 13:32:54.219940: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# For Jupyter notebooks\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import commonly used modules\n",
    "from data_utils import *\n",
    "from model_utils import *\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from numba import cuda\n",
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "from config_utils import load_config\n",
    "\n",
    "# Load default config\n",
    "CONFIG = load_config('../config/fl_template_config.yaml')\n",
    "\n",
    "# Export commonly used items\n",
    "__all__ = ['CONFIG']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd135af-d3eb-45a2-b93e-80f411e335b5",
   "metadata": {},
   "source": [
    "## FedAvg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa655466-f5d4-4223-807f-e9e0313fb255",
   "metadata": {},
   "source": [
    "### Federated Learning Averaging Pseudocode\n",
    "\n",
    "### Server Initialization:\n",
    "Initialize global model weights W₀\n",
    "\n",
    "### Main Federated Learning Loop:\n",
    "For each round t = 1 to T:\n",
    "    \n",
    "    1. Select a subset of clients Sₜ (or use all available clients)\n",
    "    \n",
    "    2. Broadcast the current global model weights Wₜ to all clients in Sₜ\n",
    "\n",
    "    3. For each client k in Sₜ (executed in parallel):\n",
    "         - Perform a local update:\n",
    "           Wₜᵏ = ClientUpdate(Wₜ, local_dataₖ)\n",
    "         - Return updated local model weights Wₜᵏ along with the number of samples nₖ\n",
    "\n",
    "    4. Aggregate the updated weights using weighted averaging (FedAvg):\n",
    "         - Compute total samples: N = Σₖ₍∈Sₜ₎ nₖ\n",
    "         - Update global model weights:\n",
    "           Wₜ₊₁ = Σₖ₍∈Sₜ₎ (nₖ / N) * Wₜᵏ\n",
    "\n",
    "Return final global model weights W_T\n",
    "\n",
    "### ClientUpdate Function:\n",
    "Function ClientUpdate(W, local_data):\n",
    "    \n",
    "    - Set W_local = W\n",
    "    \n",
    "    - For each local epoch e = 1 to E:\n",
    "         - For each batch b in local_data:\n",
    "              - Compute gradient: grad = ∇(loss(W_local, b))\n",
    "              - Update local weights: W_local = W_local - learning_rate * grad\n",
    "              \n",
    "    Return W_local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7276035-ce5b-41e2-a429-a9c4557b2a82",
   "metadata": {},
   "source": [
    "#### Central Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "034d2f87-59d1-45ea-891c-be69f127fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9c36563-db7a-4182-844d-53ca7545e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd69ae14-952a-402d-8ab0-27920d8c7b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yash/miniconda3/envs/fedml/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1742931179.246352    5842 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742931181.881267    5923 service.cc:152] XLA service 0x7fc4d000f360 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1742931181.881316    5923 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Laptop GPU, Compute Capability 8.6\n",
      "2025-03-25 13:33:01.937529: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1742931182.261379    5923 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/7\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 5s/step - accuracy: 0.7500 - auc: 0.6636 - loss: 0.6730 - precision: 0.6667 - recall: 0.4000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742931185.441751    5923 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 677ms/step - accuracy: 0.6235 - auc: 0.5452 - loss: 1.1168 - precision: 0.6877 - recall: 0.5327\n",
      "Epoch 2/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6074 - auc: 0.8511 - loss: 0.5936 - precision: 0.5811 - recall: 0.9803\n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9183 - auc: 0.9574 - loss: 0.3374 - precision: 0.9136 - recall: 0.9321\n",
      "Epoch 4/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9183 - auc: 0.9687 - loss: 0.1966 - precision: 0.9478 - recall: 0.8816\n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8842 - auc: 0.9372 - loss: 0.3660 - precision: 0.9107 - recall: 0.8725\n",
      "Epoch 6/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9032 - auc: 0.9678 - loss: 0.3291 - precision: 0.8662 - recall: 0.9607\n",
      "Epoch 7/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9322 - auc: 0.9676 - loss: 0.2258 - precision: 1.0000 - recall: 0.8670\n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9560 - auc: 0.9856 - loss: 0.1518 - precision: 1.0000 - recall: 0.9102\n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9278 - auc: 0.9924 - loss: 0.1295 - precision: 0.9248 - recall: 0.9231\n",
      "Epoch 10/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9888 - auc: 1.0000 - loss: 0.0587 - precision: 1.0000 - recall: 0.9803\n",
      "💾 Model saved to: ../experiments/fed_ml_experiment_1/models/central_model.keras\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "trainx,trainy = load_training_data(f'../experiments/{CONFIG['experiment_name']}/processed_data/init.npy')\n",
    "history =  train_model(model,trainx,trainy)\n",
    "history_dic['init'] = history\n",
    "save_model(model,f'../experiments/{CONFIG['experiment_name']}/models/central_model.keras')\n",
    "del model\n",
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310839d-f271-4020-8163-19e1f2afb120",
   "metadata": {},
   "source": [
    "#### Client Models Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e0ae36-55dd-4d7b-bcfb-6e70de3aff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Model loaded from: ../experiments/fed_ml_experiment_1/models/central_model.keras\n",
      "💾 Model saved to: ../experiments/fed_ml_experiment_1/models/client_model_1.keras\n",
      "💾 Model saved to: ../experiments/fed_ml_experiment_1/models/client_model_2.keras\n",
      "💾 Model saved to: ../experiments/fed_ml_experiment_1/models/client_model_3.keras\n",
      "💾 Model saved to: ../experiments/fed_ml_experiment_1/models/client_model_4.keras\n",
      "💾 Model saved to: ../experiments/fed_ml_experiment_1/models/client_model_5.keras\n"
     ]
    }
   ],
   "source": [
    "central_model = load_model_from_disk(f'../experiments/{CONFIG['experiment_name']}/models/central_model.keras')\n",
    "for i in range(CONFIG.get('num_clients',5)):\n",
    "    model = create_model(seed = i+1)\n",
    "    model.set_weights(central_model.get_weights())\n",
    "    model._name = f'client_model_{i+1}'\n",
    "    save_model(model,f'../experiments/{CONFIG['experiment_name']}/models/{model._name}.keras')\n",
    "    del model\n",
    "    K.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738d8c06-7d4f-4aeb-8acd-b1d2943c5696",
   "metadata": {},
   "source": [
    "#### Local Train Client models and Global Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33498a-7634-40ff-90af-68a2e204285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31dfff-d15f-48e2-8654-13f0ef91172c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## FedAvg Conditioned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988cea4-dd7a-41a2-a711-227a409e8094",
   "metadata": {},
   "source": [
    "### Modified FedAvg with Performance-based Weighting\n",
    "\n",
    "### Server Initialization:\n",
    "Initialize global model weights W₀\n",
    "\n",
    "### Main Federated Learning Loop:\n",
    "\n",
    "For each round t = 1 to T:\n",
    "\n",
    "    1. Select a subset of clients Sₜ (or use all available clients)\n",
    "    2. Broadcast the current global model weights Wₜ to all clients in Sₜ\n",
    "\n",
    "    3. For each client k in Sₜ (executed in parallel):\n",
    "         - Perform a local update:\n",
    "           Wₜᵏ = ClientUpdate(Wₜ, local_dataₖ)\n",
    "         - Evaluate the updated model on a common validation set:\n",
    "           aₖ = Evaluate(Wₜᵏ, validation_set)  # e.g., accuracy\n",
    "         - Return updated model Wₜᵏ, number of samples nₖ, and accuracy aₖ\n",
    "\n",
    "    4. Aggregate the updated weights:\n",
    "         - Compute the performance-weighted sum of samples:\n",
    "           Total_weight = Σₖ₍∈Sₜ₎ (nₖ × aₖ)\n",
    "         - Update global model weights:\n",
    "           Wₜ₊₁ = Σₖ₍∈Sₜ₎ [(nₖ × aₖ) / Total_weight] × Wₜᵏ\n",
    "\n",
    "Return final global model weights W_T\n",
    "\n",
    "### ClientUpdate Function:\n",
    "\n",
    "Function ClientUpdate(W, local_data):\n",
    "    \n",
    "    Set W_local = W\n",
    "    For each local epoch e = 1 to E:\n",
    "         For each batch b in local_data:\n",
    "              - Compute gradient: grad = ∇(loss(W_local, b))\n",
    "              - Update local weights: W_local = W_local - learning_rate * grad\n",
    "    Return W_local\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31356d21-8a4a-441d-a64a-cfff1fdcae51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Asynchronous Weight Updating Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f33ca41-ee95-4c42-9fa3-ed6f7bfa0897",
   "metadata": {},
   "source": [
    "### Federated Learning with Partial Weight Sharing (Deep Layers Updated Frequently)\n",
    "\n",
    "### Server Initialization:\n",
    "Initialize global shallow weights W_shallow₀\n",
    "Initialize global deep weights W_deep₀\n",
    "Set shallow_update_interval K  # e.g., update shallow layers every K rounds, update deep layers every round\n",
    "\n",
    "### Main Federated Learning Loop:\n",
    "\n",
    "For each round t = 1 to T:\n",
    "\n",
    "    1. Determine if this is a shallow update round:\n",
    "         If (t mod K == 0):\n",
    "             shallow_update = True\n",
    "         Else:\n",
    "             shallow_update = False\n",
    "\n",
    "    2. Client Selection & Broadcast:\n",
    "         Select a subset of clients Sₜ\n",
    "         For each client in Sₜ, send:\n",
    "             - Current deep weights: W_deepₜ  (always sent)\n",
    "             - If shallow_update is True, also send current shallow weights: W_shallowₜ\n",
    "             - Otherwise, clients use their locally stored shallow weights\n",
    "\n",
    "    3. Clients' Local Update (executed in parallel):\n",
    "         For each client k in Sₜ:\n",
    "             - If shallow_update is True:\n",
    "                  (W_shallowₜ^k, W_deepₜ^k) = ClientUpdate(W_shallowₜ, W_deepₜ, local_dataₖ, update_shallow=True)\n",
    "             - Else:\n",
    "                  (W_shallow_local, W_deepₜ^k) = ClientUpdate(W_shallow_local, W_deepₜ, local_dataₖ, update_shallow=False)\n",
    "             - Evaluate the full updated model on a common validation set:\n",
    "                  aₖ = Evaluate(FullModel(W_shallow, W_deep), validation_set)  # e.g., accuracy\n",
    "             - Return to server:\n",
    "                  - For shallow layers: if update_shallow is True, return updated W_shallowₜ^k; otherwise, no update (or the previous version)\n",
    "                  - Updated deep weights: W_deepₜ^k\n",
    "                  - Local sample count nₖ and performance metric aₖ\n",
    "\n",
    "    4. Server Aggregation:\n",
    "         # Always aggregate deep layers:\n",
    "         Compute Total_weight_deep = Σₖ₍∈Sₜ₎ (nₖ × aₖ)\n",
    "         Update global deep weights:\n",
    "             W_deepₜ₊₁ = Σₖ₍∈Sₜ₎ [ (nₖ × aₖ) / Total_weight_deep ] × W_deepₜ^k\n",
    "\n",
    "         # Aggregate shallow layers only on shallow update rounds:\n",
    "         If shallow_update is True:\n",
    "             Compute Total_weight_shallow = Σₖ₍∈Sₜ₎ (nₖ × aₖ)\n",
    "             Update global shallow weights:\n",
    "                 W_shallowₜ₊₁ = Σₖ₍∈Sₜ₎ [ (nₖ × aₖ) / Total_weight_shallow ] × W_shallowₜ^k\n",
    "         Else:\n",
    "             W_shallowₜ₊₁ = W_shallowₜ  # Keep shallow layers unchanged\n",
    "\n",
    "    Return final global model: {W_shallow_T, W_deep_T}\n",
    "\n",
    "\n",
    "### ClientUpdate Function:\n",
    "\n",
    "Function ClientUpdate(shallow_weights, deep_weights, local_data, update_shallow):\n",
    "\n",
    "    If update_shallow is True:\n",
    "         Set local_shallow = shallow_weights    # Received from server\n",
    "    Else:\n",
    "         Set local_shallow = local_shallow      # Use previously stored shallow weights locally\n",
    "\n",
    "    Set local_deep = deep_weights              # Always use the latest deep weights from server\n",
    "\n",
    "    For each local epoch e = 1 to E:\n",
    "         For each batch b in local_data:\n",
    "              If update_shallow is True:\n",
    "                  - Compute gradients for both layers:\n",
    "                        grad_shallow, grad_deep = ∇(loss(FullModel(local_shallow, local_deep), b))\n",
    "                  - Update shallow layers:\n",
    "                        local_shallow = local_shallow - learning_rate * grad_shallow\n",
    "              Else:\n",
    "                  - Compute gradient only for deep layers (shallow remains fixed):\n",
    "                        grad_deep = ∇(loss(FullModel(local_shallow, local_deep), b))\n",
    "              - Update deep layers:\n",
    "                    local_deep = local_deep - learning_rate * grad_deep\n",
    "\n",
    "    If update_shallow is True:\n",
    "         Return (local_shallow, local_deep)\n",
    "    Else:\n",
    "         Return (local_shallow, local_deep)  # Note: shallow remains unchanged from before the round\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
